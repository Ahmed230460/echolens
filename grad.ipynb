{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d655d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import cv2\n",
    "import os\n",
    "from PIL import Image\n",
    "import io\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22f2f5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_video(input_path, output_path, target_size=(224, 224)):\n",
    "    # Open the input video\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "\n",
    "    # Get video properties\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, target_size)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Resize the frame\n",
    "        resized_frame = cv2.resize(frame, target_size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        # Normalize the frame (scale pixel values to [0, 1])\n",
    "        normalized_frame = resized_frame / 255.0\n",
    "\n",
    "        # Convert back to uint8 format for video writing (0-255 range)\n",
    "        output_frame = (normalized_frame * 255).astype(np.uint8)\n",
    "\n",
    "        # Write the processed frame\n",
    "        out.write(output_frame)\n",
    "\n",
    "    # Release everything\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "input_video = r\"C:\\Users\\AHMED DAWOD\\Downloads\\graduation project\\RoadAccidents002_x264.mp4\"\n",
    "output_video = \"output_video_preprocessing.mp4\"\n",
    "preprocess_video(input_video, output_video, target_size=(224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d97defe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file  \n",
      "View Ultralytics Settings with 'yolo settings' or at 'C:\\Users\\AHMED DAWOD\\AppData\\Roaming\\Ultralytics\\settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo12n.pt to 'yolo12n.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5.34M/5.34M [00:01<00:00, 3.86MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Video FPS: 30.0\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['lap>=0.5.12'] not found, attempting AutoUpdate...\n",
      "Collecting lap>=0.5.12\n",
      "  Downloading lap-0.5.12-cp39-cp39-win_amd64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy>=1.21.6 in c:\\users\\ahmed dawod\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from lap>=0.5.12) (1.23.5)\n",
      "Downloading lap-0.5.12-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 1.5/1.5 MB 5.2 MB/s eta 0:00:00\n",
      "Installing collected packages: lap\n",
      "Successfully installed lap-0.5.12\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success  5.8s, installed 1 package: ['lap>=0.5.12']\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m  \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n",
      "Event started at frame 5 (0.17s)\n",
      "Event ended at frame 36 (1.20s)\n",
      "Event started at frame 40 (1.33s)\n",
      "Event ended at frame 71 (2.37s)\n",
      "Event started at frame 75 (2.50s)\n",
      "Event ended at frame 106 (3.53s)\n",
      "Event started at frame 110 (3.67s)\n",
      "Event ended at frame 141 (4.70s)\n",
      "Event started at frame 145 (4.83s)\n",
      "Event ended at frame 176 (5.87s)\n",
      "Event started at frame 180 (6.00s)\n",
      "Event ended at frame 216 (7.20s)\n",
      "Event started at frame 218 (7.27s)\n",
      "Event ended at frame 252 (8.40s)\n",
      "Event started at frame 253 (8.43s)\n",
      "Event ended at frame 284 (9.47s)\n",
      "Event started at frame 285 (9.50s)\n",
      "Event ended at frame 316 (10.53s)\n",
      "Event started at frame 320 (10.67s)\n",
      "Event ended at frame 346 (11.53s) - Video ended\n",
      "\n",
      "Raw keyframes video saved as: keyframes_only_output.mp4\n",
      "Annotated keyframes video saved as: keyframes_annotated_output.mp4\n",
      "Significant keyframes video saved as: significant_keyframes_output.mp4\n",
      "Total frames written to raw/annotated: 10 (displayed frames: 100)\n",
      "Total frames written to significant: 10 (displayed frames: 10)\n",
      "Output video duration (raw/annotated): 3.33 seconds at 30.0 FPS\n",
      "Output video duration (significant): 0.33 seconds at 30.0 FPS\n",
      "\n",
      "Event Summary:\n",
      "Event 1: Start 0.17s (Frame 5), End 1.20s (Frame 36)\n",
      "Event 2: Start 1.33s (Frame 40), End 2.37s (Frame 71)\n",
      "Event 3: Start 2.50s (Frame 75), End 3.53s (Frame 106)\n",
      "Event 4: Start 3.67s (Frame 110), End 4.70s (Frame 141)\n",
      "Event 5: Start 4.83s (Frame 145), End 5.87s (Frame 176)\n",
      "Event 6: Start 6.00s (Frame 180), End 7.20s (Frame 216)\n",
      "Event 7: Start 7.27s (Frame 218), End 8.40s (Frame 252)\n",
      "Event 8: Start 8.43s (Frame 253), End 9.47s (Frame 284)\n",
      "Event 9: Start 9.50s (Frame 285), End 10.53s (Frame 316)\n",
      "Event 10: Start 10.67s (Frame 320), End 11.53s (Frame 346)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Set paths\n",
    "video_path = \"output_video_preprocessing.mp4\"\n",
    "output_video_raw = \"keyframes_only_output.mp4\"\n",
    "output_video_annotated = \"keyframes_annotated_output.mp4\"\n",
    "output_video_significant = \"significant_keyframes_output.mp4\"  # New output video\n",
    "\n",
    "# Load YOLOv12 model\n",
    "model = YOLO(\"yolo12n.pt\")\n",
    "\n",
    "# Open video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "ret, prev_frame = cap.read()\n",
    "if not ret:\n",
    "    print(\"Error reading video file.\")\n",
    "    cap.release()\n",
    "    exit()\n",
    "\n",
    "# Get video properties\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "print(f\"Input Video FPS: {fps}\")\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "\n",
    "# Initialize video writers\n",
    "out_raw = cv2.VideoWriter(output_video_raw, fourcc, fps, (frame_width, frame_height))\n",
    "out_annotated = cv2.VideoWriter(output_video_annotated, fourcc, fps, (frame_width, frame_height))\n",
    "out_significant = cv2.VideoWriter(output_video_significant, fourcc, fps, (frame_width, frame_height))  # New writer\n",
    "\n",
    "# Initialize variables\n",
    "prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "frame_count = 0\n",
    "event_active = False\n",
    "event_start_frame = None\n",
    "no_motion_threshold = 30\n",
    "motion_history = []\n",
    "saved_frames = 0\n",
    "saved_significant_frames = 0  # Counter for significant frames\n",
    "events = []\n",
    "event_motion_peak = 0\n",
    "peak_frame = None\n",
    "frames_per_keyframe = 10  # For original outputs\n",
    "significant_frames_per_keyframe = 1  # For new significant output\n",
    "\n",
    "def frame_to_time(frame_num, fps):\n",
    "    return frame_num / fps\n",
    "\n",
    "def check_tracking_event(results):\n",
    "    if not results or not hasattr(results[0], 'boxes'):\n",
    "        return False\n",
    "    boxes = results[0].boxes.xyxy.cpu().numpy()\n",
    "    if len(boxes) == 0:\n",
    "        return False\n",
    "    for i in range(len(boxes)):\n",
    "        for j in range(i + 1, len(boxes)):\n",
    "            x1, y1, x2, y2 = boxes[i]\n",
    "            x3, y3, x4, y4 = boxes[j]\n",
    "            if (x1 < x4 and x2 > x3 and y1 < y4 and y2 > y3):\n",
    "                return True\n",
    "    return len(boxes) > 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, current_frame = cap.read()\n",
    "    if not ret:\n",
    "        if event_active:\n",
    "            end_time = frame_to_time(frame_count, fps)\n",
    "            print(f\"Event ended at frame {frame_count} ({end_time:.2f}s) - Video ended\")\n",
    "            events[-1][\"end_frame\"] = frame_count\n",
    "            events[-1][\"end_time\"] = end_time\n",
    "            if peak_frame is not None:\n",
    "                # Write to original outputs\n",
    "                for _ in range(frames_per_keyframe):\n",
    "                    out_raw.write(peak_frame[0])\n",
    "                    out_annotated.write(peak_frame[1])\n",
    "                saved_frames += 1\n",
    "                # Write to significant output\n",
    "                for _ in range(significant_frames_per_keyframe):\n",
    "                    out_significant.write(peak_frame[0])\n",
    "                saved_significant_frames += 1\n",
    "            event_active = False\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "    current_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Motion Detection\n",
    "    diff_gray = cv2.absdiff(prev_gray, current_gray)\n",
    "    _, thresh_gray = cv2.threshold(diff_gray, 30, 255, cv2.THRESH_BINARY)\n",
    "    non_zero_count = cv2.countNonZero(thresh_gray)\n",
    "    motion_history.append(non_zero_count)\n",
    "    if len(motion_history) > 50:\n",
    "        motion_history.pop(0)\n",
    "    adaptive_threshold = max(100, np.mean(motion_history) * 2)\n",
    "\n",
    "    diff_b = cv2.absdiff(prev_frame[:, :, 0], current_frame[:, :, 0])\n",
    "    diff_g = cv2.absdiff(prev_frame[:, :, 1], current_frame[:, :, 1])\n",
    "    diff_r = cv2.absdiff(prev_frame[:, :, 2], current_frame[:, :, 2])\n",
    "    color_diff = cv2.max(cv2.max(diff_b, diff_g), diff_r)\n",
    "    _, thresh_color = cv2.threshold(color_diff, 30, 255, cv2.THRESH_BINARY)\n",
    "    color_change = cv2.countNonZero(thresh_color)\n",
    "\n",
    "    flow = cv2.calcOpticalFlowFarneback(prev_gray, current_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    magnitude = np.sqrt(flow[..., 0]**2 + flow[..., 1]**2)\n",
    "    flow_motion = np.mean(magnitude) > 2\n",
    "\n",
    "    motion_detected = (non_zero_count > adaptive_threshold) or \\\n",
    "                      (color_change > adaptive_threshold) or \\\n",
    "                      flow_motion\n",
    "\n",
    "    # YOLOv12 + BoT-SORT Tracking\n",
    "    tracking_event = False\n",
    "    annotated_frame = current_frame.copy()\n",
    "    if frame_count % 5 == 0:\n",
    "        results = model.track(\n",
    "            source=current_frame,\n",
    "            persist=True,\n",
    "            tracker=\"botsort.yaml\",\n",
    "            conf=0.3,\n",
    "            iou=0.5,\n",
    "            verbose=False\n",
    "        )\n",
    "        tracking_event = check_tracking_event(results)\n",
    "        if results and results[0].boxes:\n",
    "            annotated_frame = results[0].plot()\n",
    "\n",
    "    # Combine Motion and Tracking\n",
    "    significant_event = motion_detected or tracking_event\n",
    "\n",
    "    # Handle Events\n",
    "    if significant_event:\n",
    "        if not event_active:\n",
    "            event_active = True\n",
    "            event_start_frame = frame_count\n",
    "            start_time = frame_to_time(frame_count, fps)\n",
    "            print(f\"Event started at frame {frame_count} ({start_time:.2f}s)\")\n",
    "            events.append({\"start_frame\": frame_count, \"start_time\": start_time})\n",
    "            event_motion_peak = 0\n",
    "            peak_frame = None\n",
    "\n",
    "        motion_score = non_zero_count + color_change + np.mean(magnitude)\n",
    "        if motion_score > event_motion_peak:\n",
    "            event_motion_peak = motion_score\n",
    "            peak_frame = (current_frame, annotated_frame)\n",
    "\n",
    "    elif event_active and (frame_count - event_start_frame) > no_motion_threshold:\n",
    "        event_active = False\n",
    "        end_time = frame_to_time(frame_count, fps)\n",
    "        print(f\"Event ended at frame {frame_count} ({end_time:.2f}s)\")\n",
    "        events[-1][\"end_frame\"] = frame_count\n",
    "        events[-1][\"end_time\"] = end_time\n",
    "        if peak_frame is not None:\n",
    "            # Write to original outputs\n",
    "            for _ in range(frames_per_keyframe):\n",
    "                out_raw.write(peak_frame[0])\n",
    "                out_annotated.write(peak_frame[1])\n",
    "            saved_frames += 1\n",
    "            # Write to significant output\n",
    "            for _ in range(significant_frames_per_keyframe):\n",
    "                out_significant.write(peak_frame[0])\n",
    "            saved_significant_frames += 1\n",
    "\n",
    "    prev_gray = current_gray\n",
    "    prev_frame = current_frame.copy()\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "out_raw.release()\n",
    "out_annotated.release()\n",
    "out_significant.release()\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\nRaw keyframes video saved as: {output_video_raw}\")\n",
    "print(f\"Annotated keyframes video saved as: {output_video_annotated}\")\n",
    "print(f\"Significant keyframes video saved as: {output_video_significant}\")\n",
    "print(f\"Total frames written to raw/annotated: {saved_frames} (displayed frames: {saved_frames * frames_per_keyframe})\")\n",
    "print(f\"Total frames written to significant: {saved_significant_frames} (displayed frames: {saved_significant_frames * significant_frames_per_keyframe})\")\n",
    "print(f\"Output video duration (raw/annotated): {saved_frames * frames_per_keyframe / fps:.2f} seconds at {fps} FPS\")\n",
    "print(f\"Output video duration (significant): {saved_significant_frames * significant_frames_per_keyframe / fps:.2f} seconds at {fps} FPS\")\n",
    "print(\"\\nEvent Summary:\")\n",
    "for i, event in enumerate(events, 1):\n",
    "    print(f\"Event {i}: Start {event['start_time']:.2f}s (Frame {event['start_frame']}), \"\n",
    "          f\"End {event['end_time']:.2f}s (Frame {event['end_frame']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aff49d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\AHMED DAWOD/.cache\\torch\\hub\\facebookresearch_pytorchvideo_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video: significant_keyframes_output.mp4\n",
      "Predicted Label: roadaccidents\n",
      "Confidence: 0.6165\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "def load_i3d_ucf_finetuned(repo_id=\"Ahmeddawood0001/i3d_ucf_finetuned\", filename=\"i3d_ucf_finetuned.pth\"):\n",
    "    class I3DClassifier(nn.Module):\n",
    "        def __init__(self, num_classes):\n",
    "            super(I3DClassifier, self).__init__()\n",
    "            self.i3d = torch.hub.load('facebookresearch/pytorchvideo', 'i3d_r50', pretrained=True)\n",
    "            self.dropout = nn.Dropout(0.3)\n",
    "            self.i3d.blocks[6].proj = nn.Linear(2048, num_classes)\n",
    "        def forward(self, x):\n",
    "            x = self.i3d(x)\n",
    "            x = self.dropout(x)\n",
    "            return x\n",
    "    device = torch.device(\"cpu\")  # Explicitly set to CPU\n",
    "    model = I3DClassifier(num_classes=8).to(device)\n",
    "    weights_path = hf_hub_download(repo_id=repo_id, filename=filename)\n",
    "    # Load with map_location to CPU\n",
    "    model.load_state_dict(torch.load(weights_path, map_location=torch.device('cpu')))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Define frame extraction function\n",
    "def extract_frames(video_path, max_frames=32, frame_size=(224, 224)):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while len(frames) < max_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = cv2.resize(frame, frame_size)\n",
    "        frames.append(frame)\n",
    "    while len(frames) < max_frames:\n",
    "        frames.append(frames[-1])\n",
    "    frames = frames[:max_frames]\n",
    "    frames = np.stack(frames)\n",
    "    frames = torch.from_numpy(frames).permute(0, 3, 1, 2).float() / 255.0\n",
    "    frames = frames.permute(1, 0, 2, 3)\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "# Define classification function\n",
    "def classify_video(video_path, model, labels):\n",
    "    frames = extract_frames(video_path)\n",
    "    frames = frames.unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(frames)\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        predicted_idx = torch.argmax(probabilities, dim=1).item()\n",
    "        predicted_label = labels[predicted_idx]\n",
    "        confidence = probabilities[0, predicted_idx].item()\n",
    "    return predicted_label, confidence\n",
    "\n",
    "# Example usage\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "labels = [\"arrest\", \"Explosion\", \"Fight\", \"normal\", \"roadaccidents\", \"shooting\", \"Stealing\", \"vandalism\"]\n",
    "model = load_i3d_ucf_finetuned()\n",
    "video_path = \"significant_keyframes_output.mp4\"  # Replace with your video path\n",
    "predicted_label, confidence = classify_video(video_path, model, labels)\n",
    "print(f\"Video: {video_path}\")\n",
    "print(f\"Predicted Label: {predicted_label}\")\n",
    "\n",
    "print(f\"Confidence: {confidence:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "774df0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded Successfully\n",
      "Extracted 10 frames\n",
      "Video Prediction: roadaccidents\n",
      "Descriptions Generated Successfully\n",
      "frames\\frame_0.jpg: Here's a one-sentence description of the image:\n",
      "\n",
      "A police car is stopped on a city street near pedestrians, possibly responding to or investigating a road accident.\n",
      "\n",
      "frames\\frame_1.jpg: Here's a one-sentence description of the image:\n",
      "\n",
      "A car has mounted the curb and appears to have collided with a pedestrian, causing people nearby to react.\n",
      "\n",
      "frames\\frame_2.jpg: That's a still image from a video showing a pedestrian walking in a marked crosswalk as cars pass by, with no apparent immediate accident.\n",
      "\n",
      "frames\\frame_3.jpg: Here's a one-sentence description of the image:\n",
      "\n",
      "A pedestrian is in danger of being hit by a vehicle on a busy city street.\n",
      "\n",
      "frames\\frame_4.jpg: That's a still image showing a pedestrian seemingly running into the street after almost being struck by a vehicle, possibly causing a near-miss accident.\n",
      "\n",
      "frames\\frame_5.jpg: A dark-colored car is speeding down a city street, narrowly missing pedestrians on the sidewalk.\n",
      "\n",
      "frames\\frame_6.jpg: A bus appears to have mounted the curb and is about to hit pedestrians.\n",
      "\n",
      "frames\\frame_7.jpg: A bus has collided with pedestrians and other objects on a busy city street.\n",
      "\n",
      "frames\\frame_8.jpg: A bus has crashed into a bus stop, causing chaos and injuring pedestrians.\n",
      "\n",
      "frames\\frame_9.jpg: That's a chaotic scene; a pedestrian appears to have been struck by a vehicle, resulting in multiple people lying on the ground amidst the commotion of other pedestrians and vehicles.\n",
      "\n",
      "\n",
      "**Final Summary:**\n",
      "\n",
      "A traffic accident involving a bus and possibly other vehicles occurred on a busy city street, resulting in multiple pedestrians being struck and injured.  The incident may have begun with a near-miss or a car mounting the curb, escalating into a major collision with significant casualties.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set up the Gemini API key\n",
    "genai.configure(api_key=\"AIzaSyCZFf2r-fmE9uRQjKebHfF_MZhDKwiZP7A\")  # Replace with your actual API key\n",
    "\n",
    "# Load the model\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "print(\"Model Loaded Successfully\")\n",
    "\n",
    "\n",
    "# Define video path & output directory\n",
    "video_path = \"significant_keyframes_output.mp4\"\n",
    "output_dir = \"frames\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Capture video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# Extract 15 evenly distributed frames\n",
    "frame_rate = 15\n",
    "step = max(1, total_frames // frame_rate)\n",
    "\n",
    "frames = []\n",
    "frame_idx = 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    if frame_idx % step == 0:\n",
    "        frame_path = os.path.join(output_dir, f\"frame_{frame_idx}.jpg\")\n",
    "        cv2.imwrite(frame_path, frame)\n",
    "        frames.append(frame_path)\n",
    "\n",
    "    frame_idx += 1\n",
    "\n",
    "cap.release()\n",
    "print(f\"Extracted {len(frames)} frames\")\n",
    "\n",
    "\n",
    "\n",
    "# Define video prediction label\n",
    "video_prediction = predicted_label\n",
    "\n",
    "print(f\"Video Prediction: {video_prediction}\")\n",
    "\n",
    "# Dictionary to store descriptions\n",
    "descriptions = {}\n",
    "\n",
    "for frame_path in frames:\n",
    "    prompt = (\n",
    "        f\"This frame is from a video classified as '{video_prediction}'. \"\n",
    "        \"Describe the event happening in the image in one sentence.\"\n",
    "    )\n",
    "\n",
    "    with open(frame_path, \"rb\") as img_file:\n",
    "        image_data = Image.open(io.BytesIO(img_file.read()))\n",
    "\n",
    "    # Ensure model is loaded\n",
    "    if model:\n",
    "        response = model.generate_content([prompt, image_data])\n",
    "        descriptions[frame_path] = response.text\n",
    "    else:\n",
    "        print(\"Error: Model is not defined.\")\n",
    "\n",
    "print(\"Descriptions Generated Successfully\")\n",
    "\n",
    "\n",
    "for frame, desc in descriptions.items():\n",
    "    print(f\"{frame}: {desc}\")\n",
    "\n",
    "\n",
    "# Create summary prompt\n",
    "summary_prompt = (\n",
    "    \"Here are multiple descriptions of frames from a surveillance video:\\n\"\n",
    "    + \"\\n\".join(descriptions.values()) +  # Combine all frame descriptions\n",
    "    \"\\nBased on these descriptions, provide a concise summary of the overall event.\"\n",
    ")\n",
    "\n",
    "# Generate summary response\n",
    "summary_response = model.generate_content(summary_prompt)\n",
    "\n",
    "# Print the final summary\n",
    "print(\"\\n**Final Summary:**\\n\")\n",
    "print(summary_response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0e9e9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
